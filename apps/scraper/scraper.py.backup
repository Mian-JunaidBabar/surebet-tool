"""
Surebet Tool - Web Scraper Service
==================================

This script dynamically scrapes betting odds from configured targets and sends the data to the backend API.
It extracts event information, odds, and deep links for the "Go to Bet" functionality.

Author: Surebet Tool Team
Version: 2.0.0
"""

import requests
import time
import re
import logging
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration Constants
BACKEND_API_URL = "http://backend:8000/api/v1/data/ingest"
SCRAPER_TARGETS_URL = "http://backend:8000/api/v1/scraper/targets?active_only=true"
BASE_DOMAIN = "https://www.betexplorer.com"

# Scraping Configuration
TIMEOUT_MS = 30000  # 30 seconds timeout
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds


def clean_text(text: str) -> str:
    """
    Clean and normalize text extracted from web elements.
    
    Args:
        text: Raw text to clean
        
    Returns:
        Cleaned text string
    """
    if not text:
        return ""
    
    # Remove extra whitespace and normalize
    cleaned = re.sub(r'\s+', ' ', text.strip())
    return cleaned


def parse_odds(odds_text: str) -> Optional[float]:
    """
    Parse odds text and convert to float.
    
    Args:
        odds_text: Raw odds text from website
        
    Returns:
        Float odds value or None if parsing fails
    """
    if not odds_text:
        return None
    
    try:
        # Clean the text and extract numeric value
        cleaned = clean_text(odds_text)
        # Remove any non-numeric characters except decimal point
        numeric_text = re.sub(r'[^\d.]', '', cleaned)
        
        if numeric_text:
            odds_value = float(numeric_text)
            # Validate odds range (typical betting odds are between 1.01 and 100)
            if 1.0 <= odds_value <= 100.0:
                return odds_value
    except (ValueError, TypeError):
        pass
    
    return None


def generate_event_id(event_name: str) -> str:
    """
    Generate a unique event ID from the event name.
    
    Args:
        event_name: The event name
        
    Returns:
        Unique event ID string
    """
    # Create a simple hash-like ID from the event name
    cleaned_name = re.sub(r'[^\w\s-]', '', event_name.lower())
    event_id = re.sub(r'\s+', '-', cleaned_name.strip())
    
    # Add timestamp to ensure uniqueness
    timestamp = str(int(time.time()))[-6:]  # Last 6 digits of timestamp
    
    return f"{event_id}-{timestamp}"


def extract_event_data(row_element, target_name: str) -> Optional[Dict[str, Any]]:
    """
    Extract all data from a single table row element.
    
    Args:
        row_element: Playwright element representing a table row
        target_name: Name of the scraping target for logging and bookmaker field
        
    Returns:
        Dictionary with extracted event data or None if extraction fails
    """
    try:
        # Extract event name
        participant_element = row_element.query_selector(".table-main__participant")
        if not participant_element:
            logger.warning("No participant element found in row")
            return None
        
        event_name = clean_text(participant_element.text_content())
        if not event_name:
            logger.warning("Empty event name found")
            return None
        
        # Extract deep link URL
        link_element = participant_element.query_selector("a")
        if not link_element:
            logger.warning(f"No link found for event: {event_name}")
            return None
        
        relative_url = link_element.get_attribute("href")
        if not relative_url:
            logger.warning(f"No href attribute found for event: {event_name}")
            return None
        
        # Create absolute URL
        deep_link_url = urljoin(BASE_DOMAIN, relative_url)
        
        # Extract odds
        odds_elements = row_element.query_selector_all(".table-main__odds")
        if len(odds_elements) < 3:
            logger.warning(f"Not enough odds found for event: {event_name} (found {len(odds_elements)})")
            return None
        
        # Extract Home (1), Draw (X), Away (2) odds
        home_odds_text = odds_elements[0].text_content() if len(odds_elements) > 0 else ""
        draw_odds_text = odds_elements[1].text_content() if len(odds_elements) > 1 else ""
        away_odds_text = odds_elements[2].text_content() if len(odds_elements) > 2 else ""
        
        # Parse odds to float
        home_odds = parse_odds(home_odds_text)
        draw_odds = parse_odds(draw_odds_text)
        away_odds = parse_odds(away_odds_text)
        
        # Validate that we have all three odds
        if None in [home_odds, draw_odds, away_odds]:
            logger.warning(f"Invalid odds for event: {event_name}")
            logger.warning(f"  Home: {home_odds_text} -> {home_odds}")
            logger.warning(f"  Draw: {draw_odds_text} -> {draw_odds}")
            logger.warning(f"  Away: {away_odds_text} -> {away_odds}")
            return None
        
        # Generate unique event ID
        event_id = generate_event_id(event_name)
        
        # Structure data according to backend schema
        event_data = {
            "event_id": event_id,
            "event": event_name,
            "sport": "Football",
            "outcomes": [
                {
                    "bookmaker": target_name,
                    "name": "Home Win",
                    "odds": home_odds,
                    "deep_link_url": deep_link_url
                },
                {
                    "bookmaker": target_name,
                    "name": "Draw",
                    "odds": draw_odds,
                    "deep_link_url": deep_link_url
                },
                {
                    "bookmaker": target_name,
                    "name": "Away Win",
                    "odds": away_odds,
                    "deep_link_url": deep_link_url
                }
            ]
        }
        
        logger.info(f"âœ… Extracted: {event_name}")
        logger.info(f"  Odds: {home_odds} | {draw_odds} | {away_odds}")
        logger.info(f"  Link: {deep_link_url}")
        
        return event_data
        
    except Exception as e:
        logger.error(f"Error extracting data from row: {str(e)}")
        return None


def scrape_betting_data(target_url: str, target_name: str) -> List[Dict[str, Any]]:
    """
    Main scraping function using Playwright for a specific target URL.
    
    Args:
        target_url: The URL to scrape
        target_name: Name of the target for logging
    
    Returns:
        List of structured event data dictionaries
    """
    scraped_data = []
    
    logger.info(f"ðŸš€ Starting scrape for: {target_name}")
    logger.info(f"ðŸŽ¯ Target URL: {target_url}")
    
    with sync_playwright() as p:
        browser = None
        try:
            logger.info("ðŸŒ Launching browser...")
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # Set user agent to avoid bot detection
            page.set_extra_http_headers({
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            })
            
            logger.info(f"ðŸ”„ Navigating to {target_url}...")
            page.goto(target_url, wait_until="domcontentloaded")
            
            logger.info("â³ Waiting for odds table to load...")
            try:
                page.wait_for_selector(".table-main--leaguefixtures", timeout=TIMEOUT_MS)
                logger.info("âœ… Odds table loaded successfully")
            except PlaywrightTimeoutError:
                logger.error("âŒ Timeout waiting for odds table to load")
                return scraped_data
            
            # Give a moment for dynamic content to fully load
            time.sleep(2)
            
            logger.info("ðŸ” Finding match rows...")
            match_rows = page.query_selector_all(".table-main__row")
            
            if not match_rows:
                logger.warning("âŒ No match rows found on the page")
                return scraped_data
            
            logger.info(f"ðŸ“Š Found {len(match_rows)} match rows to process")
            
            # Process each row
            for idx, row in enumerate(match_rows, 1):
                logger.info(f"[{idx}/{len(match_rows)}] Processing row...")
                
                event_data = extract_event_data(row, target_name)
                if event_data:
                    scraped_data.append(event_data)
                else:
                    logger.warning(f"Skipped row {idx} due to extraction failure")
            
            logger.info(f"ðŸŽ‰ Scraping completed! Successfully extracted {len(scraped_data)} events")
            
        except PlaywrightTimeoutError as e:
            logger.error(f"Playwright timeout error: {str(e)}")
        except Exception as e:
            logger.error(f"Unexpected error during scraping: {str(e)}")
            
        finally:
            if browser:
                logger.info("ðŸ”’ Closing browser...")
                browser.close()
    
    return scraped_data


def send_data_to_backend(scraped_data: List[Dict[str, Any]]) -> bool:
    """
    Send scraped data to the backend API.
    
    Args:
        scraped_data: List of structured event dictionaries
        
    Returns:
        True if successful, False otherwise
    """
    if not scraped_data:
        logger.warning("No data to send to backend")
        return False
    
    logger.info(f"ðŸ“¤ Sending {len(scraped_data)} events to backend...")
    logger.info(f"ðŸŽ¯ Backend URL: {BACKEND_API_URL}")
    
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.post(
                BACKEND_API_URL,
                json=scraped_data,
                headers={
                    "Content-Type": "application/json",
                    "User-Agent": "Surebet-Scraper/2.0.0"
                },
                timeout=30
            )
            
            if response.status_code == 200:
                response_data = response.json()
                logger.info("âœ… Data sent successfully!")
                logger.info(f"ðŸ“Š Backend response: {response_data.get('message', 'No message')}")
                logger.info(f"ðŸ“ˆ Events processed: {response_data.get('events_processed', 'Unknown')}")
                logger.info(f"ðŸ”„ Status: {response_data.get('status', 'Unknown')}")
                return True
            else:
                logger.error(f"Backend returned error status: {response.status_code}")
                logger.error(f"Response: {response.text}")
                
        except requests.exceptions.Timeout:
            logger.warning(f"Request timeout (attempt {attempt + 1}/{MAX_RETRIES})")
        except requests.exceptions.ConnectionError:
            logger.error(f"Connection error (attempt {attempt + 1}/{MAX_RETRIES})")
            logger.error("Make sure the backend service is running!")
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error (attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}")
        except Exception as e:
            logger.error(f"Unexpected error sending data (attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}")
        
        if attempt < MAX_RETRIES - 1:
            logger.info(f"â³ Retrying in {RETRY_DELAY} seconds...")
            time.sleep(RETRY_DELAY)
    
    logger.error(f"Failed to send data after {MAX_RETRIES} attempts")
    return False


def fetch_scraper_targets() -> List[Dict[str, Any]]:
    """
    Fetch active scraper targets from the backend API.
    
    Returns:
        List of active scraper target dictionaries with 'url', 'name', and 'is_active' fields
    """
    logger.info(f"ðŸ“¥ Fetching scraper targets from backend...")
    logger.info(f"ðŸŽ¯ URL: {SCRAPER_TARGETS_URL}")
    
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(
                SCRAPER_TARGETS_URL,
                headers={
                    "User-Agent": "Surebet-Scraper/2.0.0"
                },
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                targets = data.get('targets', [])
                logger.info(f"âœ… Fetched {len(targets)} active scraper target(s)")
                
                for target in targets:
                    logger.info(f"  - {target.get('name')}: {target.get('url')}")
                
                return targets
            else:
                logger.error(f"Backend returned error status: {response.status_code}")
                logger.error(f"Response: {response.text}")
                
        except requests.exceptions.Timeout:
            logger.warning(f"Request timeout (attempt {attempt + 1}/{MAX_RETRIES})")
        except requests.exceptions.ConnectionError:
            logger.error(f"Connection error (attempt {attempt + 1}/{MAX_RETRIES})")
            logger.error("Make sure the backend service is running!")
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error (attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}")
        except Exception as e:
            logger.error(f"Unexpected error fetching targets (attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}")
        
        if attempt < MAX_RETRIES - 1:
            logger.info(f"â³ Retrying in {RETRY_DELAY} seconds...")
            time.sleep(RETRY_DELAY)
    
    logger.error(f"Failed to fetch targets after {MAX_RETRIES} attempts")
    return []


def main():
    """
    Main function that orchestrates the scraping and data sending process.
    Fetches active targets from backend and scrapes each one.
    """
    logger.info("=" * 60)
    logger.info("ðŸŽ° SUREBET TOOL - WEB SCRAPER")
    logger.info("=" * 60)
    logger.info(f"â° Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        # Step 1: Fetch active scraper targets from backend
        targets = fetch_scraper_targets()

        if not targets:
            logger.warning("âŒ No active scraper targets found. Exiting...")
            return

        # Step 2: Loop through each target and scrape data
        all_scraped_data = []

        for idx, target in enumerate(targets, 1):
            target_url = target.get('url')
            target_name = target.get('name', 'Unknown')

            if not target_url:
                logger.warning(f"âš ï¸  Skipping target {idx}: Missing URL")
                continue

            logger.info(f"\n{'=' * 60}")
            logger.info(f"ðŸ“ Processing target {idx}/{len(targets)}: {target_name}")
            logger.info(f"{'=' * 60}")

            # Scrape data from this target
            scraped_data = scrape_betting_data(target_url, target_name)

            if scraped_data:
                logger.info(f"âœ… Successfully scraped {len(scraped_data)} events from {target_name}")
                all_scraped_data.extend(scraped_data)
            else:
                logger.warning(f"âš ï¸  No data scraped from {target_name}")

            # Add a small delay between targets to be respectful
            if idx < len(targets):
                logger.info("â³ Waiting 3 seconds before next target...")
                time.sleep(3)

        # Step 3: Send all scraped data to backend
        if not all_scraped_data:
            logger.warning("\nâŒ No data was scraped from any target. Exiting...")
            return

        logger.info(f"\n{'=' * 60}")
        logger.info(f"ðŸ“Š Total events scraped: {len(all_scraped_data)}")
        logger.info(f"{'=' * 60}")

        success = send_data_to_backend(all_scraped_data)

        if success:
            logger.info("\nðŸŽ‰ Scraper completed successfully!")
        else:
            logger.error("\nâŒ Scraper completed with errors")

    except KeyboardInterrupt:
        logger.warning("\nðŸ›‘ Scraper interrupted by user")
    except Exception as e:
        logger.error(f"\nðŸ’¥ Fatal error: {str(e)}", exc_info=True)
    finally:
        logger.info(f"â° Finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)


if __name__ == "__main__":
    main()